<!DOCTYPE html>
<html>
    <head>
        <title>Choices</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" href="./Resources/Images/favicon.png">
        <link rel="stylesheet" href="./Resources/CSS/main.css">
    </head>
    <body>
        <div class="grid-container">
            <!--Navigation bar-->
            <nav>
                <a href="./index.html">Topic</a>
                <a href="./opportunities.html">Opportunities</a>
                <a href="./risks.html">Risks</a>
                <a href="./choices.html">Choices</a>
                <a href="./references.html">References</a>
            </nav>

            <!--body-->
            <main>
                <h1>Social Media Algorithm Choices</h1>
                <p>The rise of algorithm-driven content delivery systems on social media has transformed how users interact with information and one another. However, these systems raise fundamental ethical and societal questions. Among the most pressing are issues concerning personalisation versus information diversity, engagement optimisation versus well-being, and algorithmic transparency versus proprietary control. Each of these involves complex trade-offs between user experience, corporate interests, and the broader public good.</p>
                <img src="./Resources/Images/socialmedialikes.jpg" alt="Person receiving likes">
                <section>
                    <h2>Personalisation vs Information Diversity</h2>
                    <p>One of the key debates in algorithmic design revolves around whether content should be personalised to reflect users' past behaviour or diversified to introduce opposing viewpoints. Personalisation has proven beneficial in boosting user satisfaction and platform engagement. Companies like Facebook, TikTok, and YouTube have thrived by tailoring content feeds based on users' interests and behaviours, resulting in longer session times and improved ad targeting.</p>
                    <p>However, this personalised approach also poses serious risks. By reinforcing users' pre-existing beliefs and preferences, algorithms can create "filter bubbles" and "echo chambers" that isolate individuals from diverse perspectives. Such environments exacerbate ideological segregation, undermining democratic discourse. Research published by Emerald (Baird & Parasnis, 2011) and SAGE Journals (Khan et al., 2014) further supports this claim, noting that exposure to similar viewpoints contributes to increasing political polarisation. A critical perspective suggests that while personalisation improves user experience, algorithms should also be designed with systems that introduce content users might not normally encounter in order to promote ideological diversity and reduce social division.</p>
                </section>
                <section>
                    <h2>Engagement Optimisation vs Well-being</h2>
                    <p>Another central concern in algorithmic design is whether platforms should optimize for engagement metrics such as likes, shares, and watch time, or prioritise users' mental and emotional well-being. Engagement-based models are undeniably effective in driving growth. By encouraging frequent interactions, they generate a wealth of user-generated content and boost advertising revenue.</p>
                    <p>Yet this relentless focus on engagement comes at a cost. Algorithms tend to favor content that elicits strong emotional reactions - whether outrage, fear, or excitement - over that which fosters reflection or learning. ScienceDirect highlights how this design logic promotes sensational and polarising content, contributing to anxiety, misinformation, and digital burnout (Everett, 2010). From a critical standpoint, this approach commodifies user attention without accounting for its psychological toll. An alternative model would be to incorporate well-being indicators (such as screen time moderation, emotional response surveys, or content fatigue metrics) into algorithmic decisions, thereby aligning technological design with human flourishing.</p>
                </section>
                <section>
                    <h2>Algorithmic Transparency vs Proprietary Control</h2>
                    <p>A significant tension also exists between the need for algorithmic transparency and the desire of companies to maintain proprietary control over their systems. Keeping algorithms private is beneficial for protecting intellectual property and preventing manipulation by bad actors. Tech companies argue that full transparency could make platforms more susceptible to gaming or malicious exploitation.</p>
                    <p>However, this lack of openness erodes public trust and hinders the ability of users and researchers to understand how content is curated. SpringerLink notes that without transparency, it is nearly impossible to assess the societal harms of algorithmic systems - including the spread of disinformation and the reinforcement of biases (Tikhonov & Konovalova, 2020). A balanced approach would involve disclosing general principles and values that guide algorithmic behaviour while allowing third-party auditors to assess system impacts. Such a model protects proprietary details but still ensures accountability.</p>
                </section>
                <img src="./Resources/Images/algorithms.jpg" alt="Social media algorithms">
                <section>
                    <h2>User Control vs Passive Consumption</h2>
                    <p>The issue of user agency in content curation is equally important. Currently, most platforms offer a default, algorithmically curated feed - creating a smooth and low-effort experience. This appeals to the majority of users who may not have the time or expertise to fine-tune their settings.</p>
                    <p>However, the lack of customisation options encourages passive consumption of content, which may not align with users' deeper values or interests. Research from Taylor & Francis highlights how this reduces digital agency, especially among marginalised communities attempting to navigate platform dynamics and representation (Turban et al., 2011). To address this, platforms should offer more transparent and accessible tools for users to control their feeds. Options could include toggling between chronological and algorithmic views, selecting preferred topics, or adjusting the weight of certain content types. Empowering users in this way fosters digital literacy and personal agency in an increasingly automated environment.</p>
                </section>
                <section>
                    <h2>Algorithmic Promotion vs Human Moderation</h2>
                    <p>Another challenge is deciding whether content promotion should rely entirely on algorithmic processes or be guided by human oversight. Algorithms can rapidly process vast quantities of data, making them a necessity for large-scale platforms like YouTube or Instagram. This scalability reduces labour costs and allows for near-instantaneous content delivery.</p>
                    <p>Yet, the automation of content curation often leads to harmful outcomes. Algorithms may fail to detect dangerous trends, misinformation, or abusive content until after it has gone viral. One example is the tragic "Run It Straight" challenge, reported by The Guardian, which resulted in real-world harm due to unchecked algorithmic amplification (Corlett, 2025). A hybrid model that combines algorithmic efficiency with human judgment - especially for trending or flagged content - offers a more responsible approach. Human moderators bring contextual understanding and ethical reasoning that algorithms currently lack.</p>
                </section>
                <section>
                    <h2>Commercial Gain vs Public Responsibility</h2>
                    <p>The final issue involves the balance between monetising user engagement and safeguarding societal well-being. Social media platforms are largely funded by advertising, which depends on capturing and maintaining user attention. This model enables free access to services and fuels innovation through data-driven targeting.</p>
                    <p>However, when revenue becomes the sole objective, platforms often prioritise content that maximises user retention - regardless of its truthfulness, safety, or social impact. Studies from SpringerLink (Naslund et al., 2020) and Emerald (Baird & Parasnis, 2011) emphasise how commercial imperatives can override ethical considerations, leading to data exploitation, behavioral manipulation, and erosion of public trust. A critical view argues for regulatory oversight similar to the standards in finance or healthcare. This includes algorithmic audits, privacy protections, and enforceable norms around ethical design. Such measures would help ensure that platforms serve the public interest, not just their shareholders.</p>
                </section>
                <img src="./Resources/Images/algorithmsmakemoney.jpg" alt="Algorithms making money">
                <section>
                    <h2>Conclusion</h2>
                    <p>In summary, the design and implementation of social media algorithms are far from neutral. They involve deliberate choices that reflect and shape our collective values. Based on existing research and real-world evidence, several guiding principles emerge. First, platforms should embrace transparency with accountability, disclosing enough about their systems to allow meaningful oversight while protecting sensitive infrastructure. Second, algorithmic systems must focus more towards well-being - integrating metrics that account for psychological and social health. Finally, users should be empowered with tools and knowledge that enable them to take control of their digital experiences.</p>
                    <p>These steps are not simply technical adjustments; they are foundational to ensuring that digital platforms support a more informed, healthy, and democratic society. As algorithms increasingly seep themselves into public life, the ethical choices made in their design will profoundly shape the future of human interaction and information exchange.</p>
                </section>
            </main>

            <!--footer-->
            <footer>
                <span>This website is for COMP501 Group 96 Semester 1 2025</span>
            </footer>

        </div>
    </body>
</html>